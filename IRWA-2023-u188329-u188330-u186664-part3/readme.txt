The code that we have submitted has the following structure:
	1. Part 1: here you can find the implementation of the part one we delivered on october (21/10). Here are the necessary library imports as well as pre-processing and data analyzing functions. For this part some documents have been opened in the notebook by reading them from github (Rus_Ukr_war_data.json) and others from drive (Rus_Ukr_war_data_ids.csv, Evaluation_gt.csv and evaluation_own_queries.csv). These 3 documents can be found on this folder.

	2. Part 2: here you can find the implementation of the second part delivered on october (29/10). It is divided by indexing and evaluating, but first of all there is a reimplementation of the build_terms function (pre-processing function). More details can be found on the corresponding GitHub folder (IRWA-2023-u188329-u188330-u186664-part2).
	
	3. Part 3: the third part of the project is divided into three sections: classical ranking, own ranking and Word2Vec. We find important to remark that in this part we have implemented conjunctive queries, so our search engines will only return those documents that have all query terms. Additionally, the outputting structure will contain, among different information, the following fields: Tweet | Username | Date | Hashtags | Likes | Retweets | Url. To this end, we have created a function "get_hashtags()" that returns the hashtags in the tweet. It can be found at the beginning of the Part 3.
		
		3.1. Classical Ranking: here we implement the classical ranking that we have already seen in previous parts of this project. Using the tf-idf weights of the query and documents, we compute the cosine similarity and return the top-10 relevant documents for the manually inputted query. To this end, we define a function that allows us to create the index. Next step is to create the index, as well as creating functions to rank the documents and another one that allows us to search in the index. Finally, last cell of this section asks for a query and retrieves the corresponding relevant documents. It ranks them using tf-idf weights.
		
		3.2. Own scoring: in this part of the project we have designed and implemented our own scoring algorithm. We will now also take into consideration the number of likes and retweets that tweets have, apart from the previous tf-idf weights. This way we are able to quantitatively measure how viral it is and rank the tweets taking this information into consideration too. The structure is very similar than before: we create a scoring function that returns the tweets ranked using our new score system, which now includes a new dimension in the vectors which is the popularity score, and we also create a search function that retrieves the tweets accordingly. Last cell of this section allows the user to search relevant documents for whichever query he inputs. Nevertheless, we also use a function created by ourselves (min_max_scaling), which allows us to normalize the dictionary passed as parameter.

		3.3. Word2Vec: this second exercise of the third part of the project consists on ranking relevant documents using Word2Vec and cosine similarity. To this end, we create a model using all the tweet's vocabulary that allows us to represent words and tweets as vectors. The first function that we design is "get_tweet_vector()", which given a tweet, returns its vector representation. That is, the average of the tweet's words vector representation. Before applying PCA, using the elbow-graphic we determine the number of components we will be using. Finally, we create a function that, given a query, it returns its relevant documents ranked using Word2Vec +  Cosine Similarity. What this function does is, given a query, return all relevant documents. We store the tweets indexes and their vector representation in a dictionary, which will be later be used to compute the pca vector representation. After doing so, we compute the vector representation of the query: after preprocessing it (remove stop words, lowercase all letters,...) because tweets are also preprocessed, we compute its pca representation. We are now ready to compute the cosine similarity values for each tweet and query vector representation. We finally output the ranked results for our five queries using the structure that we have been using throughout all the part 3, an upgraded version with respect to the one used on previous parts.

This notebook ends here, as question 3 of the Part 3 is answered in the report.


All functiones are explained in the notebook, as well as clarifying comments.
We have commented all independent previous cells that aren't part of this third delivery in order to accelerate the running time of our notebook.

