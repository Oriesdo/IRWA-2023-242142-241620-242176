The code that we have submitted has the following structure:
	1. Part 1: here you can find the implementation of the part one we delivered last week (21/10). Here are the necessary library imports as well as pre-processing and data analyzing functions. For this part some documents have been opened in the notebook by reading them from github (Rus_Ukr_war_data.json) and others from drive (Rus_Ukr_war_data_ids.csv, Evaluation_gt.csv and evaluation_own_queries.csv). These 3 documents can be found on this folder.
	2. Part 2: it is divided by indexing and evaluating, but first of all there is a reimplementation of the build_terms function (pre-processing function). In this second part of the project we also remove # and @ because we want to considere these type of words not only when implementing the indexes, but also when querying. 
		2.1. Indexing: here we start by implementing the required functions to create a non-score based index: create_index(...) and search(...). The first one creates the index and the second one allows us to search it. You can test that it works by uncommenting the cell that follows the search(...) one. It will ask for an input and it returns 10 documents that contain, at least, one query term. The index must be created by calling the create_index(...) function. It will be done automatically if the code is runned sequentially. 
	Next step is to create a score based index. We do so by implementing a create_index_tfidf(...) function, which creates a index based on tf-idf weights. rank_documents(...) and search_tf_idf(...) functions are necessary too for this new type of index. Again, you can test that it works by uncommenting the next cell. It will ask for a query and it will return the top-10 ranked tweets matching that query, as well as their score. 
		2.2. Evaluation: we start by creating a dataframe with the prediction scores of each of the documents (evaluation_gt.csv file). Additionally, there will be a column for the label and predicted scores for each query. For each document we then fill this information. Next step is to define the evaluation techniques functions. For each query we then fill its prediction_score for its corresponding query. The other prediction_scores will be 0. Finally, we are able to call the evaluation metrics functions and we get the results. Next step is to compute the metrics for our own queries and ground truths. To this end, we open our ground truth .csv file and we randomly fill the rows that are irrelevant documents, making sure that those documents haven't yet appeared in the dataframe. The following steps are the sames than before: create new columns for each different query, fill them with their corresponding values and call the evaluation metric functions to get the results. The end of this section is the 2-dimensional representation of the returned tweets of the five queries we defined.

All functiones are explained in the notebook, as well as clarifying comments.
	
